{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50aaf295-9802-402c-a3bc-1517c458f0e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nMachine Learning part \\n\\n@Author : Pierre CLaver \\nDate:2023.12.11 \\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Machine Learning part \n",
    "\n",
    "@Author : Pierre CLaver \n",
    "Date:2024.12.11 \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#\n",
    "# Install the package watex to get the advantages of the ML utilites\n",
    "# ! pip install watex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bddee329-62a1-47fa-881c-e6e7e888b0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "#import joblib\n",
    "#pd.__version__\n",
    "\n",
    "#joblib.__version__\n",
    "#import xgboost as xgb\n",
    "#xgb.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc94f8b3-adf4-4540-9516-47acdf995772",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e96ccb84-f02c-4608-8a76-1c4b43a1696f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356bd6b2-8d59-463b-846c-70c383af4493",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aaf7fc46-0cfd-49b2-8bc6-ef571846b920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import requiered modules\n",
    "# XXX IMPORTANT ! (1)\n",
    "import warnings\n",
    "import contextlib\n",
    "import copy\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "import watex as wx\n",
    "from watex.utils import naive_scaler  # naive_imputer,\n",
    "from watex.utils import savejob  # make_naive_pipe,\n",
    "from watex.utils.funcutils import smart_format, is_iterable, is_in_if\n",
    "# from watex.utils import bin_counting, bi_selector\n",
    "# from watex.exlib import (\n",
    "#     train_test_split,\n",
    "\n",
    "# )\n",
    "from watex.exlib import (\n",
    "    KNeighborsClassifier,\n",
    "    DecisionTreeClassifier,\n",
    "    LogisticRegression,\n",
    "    SVC,\n",
    "    RandomForestClassifier,\n",
    "    AdaBoostClassifier,\n",
    "    # StandardScaler,\n",
    "    # Normalizer,\n",
    "    # RobustScaler,\n",
    "    # MinMaxScaler,\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "\n",
    ")\n",
    "from watex.models.validation import getGlobalScores\n",
    "from watex.utils.box import Boxspace\n",
    "from watex.exlib.gbm import XGBClassifier\n",
    "from watex.utils.validator import get_estimator_name\n",
    "# from sklearn.discriminant_analysis import LDA\n",
    "# from sklearn.linear_model import Ridge\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import f1_score  # roc_auc_score,\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "# # GRID PARAMS\n",
    "# from scipy.stats import expon  # uniform, randint\n",
    "from skopt.space import Categorical, Integer, Real\n",
    "from skopt.searchcv import BayesSearchCV\n",
    "# import time\n",
    "\n",
    "# from sklearn.\n",
    "from lightgbm import LGBMClassifier\n",
    "# import CatBoost\n",
    "# from pprint import pprint\n",
    "#\n",
    "# * Preprocessing\n",
    "# loading the dataset\n",
    "# p\n",
    "data = wx.read_data('alsani.csv',\n",
    "                    #  sanitize=True\n",
    "                    )\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7ab3b25-0c8c-4aa7-8cbe-50de8c01d122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions utilities\n",
    "\n",
    "# XXX IMPORTANT ! (2)\n",
    "\n",
    "\n",
    "def scale_data(d, scaler, save=False, filename=None, **scaler_kws):\n",
    "    \"\"\"Scale data using sklearn scaling estimator  \n",
    "\n",
    "    Parameters \n",
    "    ------------\n",
    "    d: Arraylike of pandas Dataframe \n",
    "       Arraylike of DataFRame containing the valid numeric data. \n",
    "\n",
    "    d: :class:`~sklearn.preprocessing.*` \n",
    "       The scaling estimator . It could be `StandardScaler`, `RobustScaler` , \n",
    "       `Normalizer` or `RobustScaler` etc \n",
    "\n",
    "    \"\"\"\n",
    "    # sc = scaler ( **scaler_kws )\n",
    "    # X_transf = sc.fit_transform ( d )\n",
    "    # if hasattr ( sc, 'feature_names_in_') :\n",
    "    #     X_transf = pd.DataFrame ( X_transf, columns = sc.feature_names_in_)\n",
    "\n",
    "    # or use the simple function below\n",
    "    try:\n",
    "        X_transf = naive_scaler(d, kind=scaler, **scaler_kws)\n",
    "    except:\n",
    "        sc = scaler(**scaler_kws)\n",
    "        X_transf = sc.fit_transform(d)\n",
    "        if hasattr(sc, 'feature_names_in_'):\n",
    "            X_transf = pd.DataFrame(X_transf, columns=sc.feature_names_in_)\n",
    "\n",
    "    if save:\n",
    "        filename = filename or get_estimator_name(scaler)\n",
    "        # remove the'csv' extension if given\n",
    "        filename = filename.replace('.csv', '')\n",
    "        X_transf.to_csv(f'{filename}.csv', index=False)\n",
    "\n",
    "    return X_transf\n",
    "\n",
    "\n",
    "class EstimatorError (BaseException):\n",
    "    pass\n",
    "\n",
    "\n",
    "def make_Xy_scale_data(*X, y, scalers: list, save=False, filename='DATA'):\n",
    "    \"\"\" Scale the data and aggregate numerical and categorical features into \n",
    "    a single dataset X, y. \n",
    "    If ``save=True`` then picked the data. \n",
    "\n",
    "    Parameters \n",
    "    ---------\n",
    "    X: List of Arraylike , pd.DataFrame \n",
    "      It can be Xnumerical and categorical  \n",
    "    scalers: Type of Sklearn Scaler estimators \n",
    "\n",
    "    save: bool, default=False \n",
    "      Pickle the data or save to Joblib \n",
    "    filename: str, \n",
    "      Name of pickled file \n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Xnum, Xcat = X\n",
    "    # X = pd.concat ([*X], axis =1 )\n",
    "    data_pickled = {}\n",
    "    for scaler in scalers:\n",
    "        #\n",
    "        X0 = pd.concat([scale_data(Xnum, scaler=scaler), Xcat], axis=1)\n",
    "        data_pickled[get_estimator_name(scaler)] = (X0, y)\n",
    "    if save:\n",
    "        savejob(data_pickled, savefile=filename)\n",
    "\n",
    "    return data_pickled if not save else None\n",
    "\n",
    "\n",
    "def reduce_Xy(\n",
    "    X,\n",
    "    components,\n",
    "    save=False,\n",
    "    filename='PCADATA',\n",
    "    ** reduce_kws\n",
    "):\n",
    "    \"\"\" Reduce X with PCA \n",
    "\n",
    "    Parameters \n",
    "    --------------\n",
    "    X: ArrayLike or pd.DataFrame \n",
    "      Array or Dataframe withs shape (n_samples, n_features ) \n",
    "    components: list, ArrayLike of int \n",
    "       List of components \n",
    "\n",
    "    filename: str, default ='PCADATA'\n",
    "       Name of pickled file \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    data_reduced = {}\n",
    "    for comp in components:\n",
    "\n",
    "        pca = PCA(n_components=comp, ** reduce_kws)\n",
    "        X_red = pca.fit_transform(X)\n",
    "\n",
    "        data_reduced['comp{:02}'.format(comp)] = X_red\n",
    "\n",
    "    if save:\n",
    "        savejob(data_reduced, savefile=filename)\n",
    "\n",
    "    return data_reduced if not save else None\n",
    "\n",
    "\n",
    "def fetch_pickled_data(file):\n",
    "    \"\"\" Get the data from the binary disk \"\"\"\n",
    "    return joblib.load(file)\n",
    "\n",
    "\n",
    "def optimization(X, y, cv,  estimator,\n",
    "                 param_grid, optimizer=GridSearchCV,\n",
    "                 scoring=\"accuracy\",  verbose=0,\n",
    "                 return_o=True, return_both=False,\n",
    "                 comp_name=None,\n",
    "                 **optimizer_kws\n",
    "\n",
    "                 ):\n",
    "    \"\"\" CRoss validate the data using the optimiser \"\"\"\n",
    "\n",
    "    N_ITER = 10\n",
    "    N_jOBS = -1\n",
    "    if get_estimator_name(optimizer) == 'BayesSearchCV':\n",
    "        opm = optimizer(estimator, search_spaces=param_grid, cv=cv,\n",
    "                        scoring=scoring,\n",
    "                        n_iter=N_ITER,\n",
    "                        n_jobs=N_jOBS,\n",
    "                        **optimizer_kws)\n",
    "    elif get_estimator_name(optimizer) == 'RandomizedSearchCV':\n",
    "        opm = optimizer(\n",
    "            estimator,\n",
    "            param_distributions=param_grid,\n",
    "            cv=cv,\n",
    "            scoring=scoring,\n",
    "            n_iter=N_ITER,\n",
    "            n_jobs=N_jOBS,\n",
    "            **optimizer_kws\n",
    "\n",
    "        )\n",
    "    else:\n",
    "        opm = optimizer(estimator, param_grid=param_grid,\n",
    "                        cv=cv, scoring=scoring,\n",
    "                        n_jobs=N_jOBS,\n",
    "                        **optimizer_kws)\n",
    "\n",
    "    opm.fit(X, y)\n",
    "\n",
    "    # Params that we need\n",
    "    dict_params = dict(\n",
    "        best_params_=opm.best_params_,\n",
    "        best_estimator_=opm.best_estimator_,\n",
    "        cv_results_=opm.cv_results_,\n",
    "    )\n",
    "\n",
    "    if verbose:\n",
    "        print(\"+\"*90)\n",
    "        print(\"{:<20}:{:>45}\".format(\n",
    "            \"Estimator {}\".format(comp_name), get_estimator_name(estimator)))\n",
    "        print(\"{:<20}:{:>45}\".format(\n",
    "            \"Optimizer\", get_estimator_name(optimizer)))\n",
    "        print(\"{:<20}:{:>45}\".format(\"CV\", cv))\n",
    "        print(\"{:<20}:{:>45}\".format('Scoring', scoring))\n",
    "        print(\"+\"*90)\n",
    "\n",
    "    o = copy.deepcopy(dict_params)\n",
    "\n",
    "    if return_both:\n",
    "        return_o = True\n",
    "    if return_o:\n",
    "        o = Boxspace(**dict_params)\n",
    "\n",
    "    if return_both:\n",
    "        o = (o, dict_params)\n",
    "\n",
    "    return o\n",
    "\n",
    "\n",
    "def evaluator(X, y, estimators, grid_params,  optimizers, cvs,\n",
    "              return_o=True, verbose=0, return_both=False, comp_name=None,\n",
    "              ):\n",
    "    # evaluate multiples estimator with parameters\n",
    "    map_estimators = {\n",
    "        \"LogisticRegression\": \"LR\",\n",
    "        'KNeighborsClassifier': 'KNN',\n",
    "        'DecisionTreeClassifier': 'DT',\n",
    "        'SVC': \"SVM\",\n",
    "        'RandomForestClassifier': \"RF\",\n",
    "        'AdaBoostClassifier': \"ADA\",\n",
    "        'XGBClassifier': \"XGB\",\n",
    "        'GaussianNB': \"NB\",\n",
    "        \"LGBMClassifier\": \"LGBM\",\n",
    "        \"LinearDiscriminantAnalysis\": \"LDA\",\n",
    "        \"Ridge\": \"Ridge\"\n",
    "    }\n",
    "\n",
    "    cv_seekers = {\"GridSearchCV\": \"GSCV\",\n",
    "                  \"RandomizedSearchCV\": \"RSCV\",\n",
    "                  \"BayesSearchCV\": \"BSCV\"\n",
    "                  }\n",
    "\n",
    "    if return_both:\n",
    "        return_o = True\n",
    "    # DICT = {}\n",
    "    estimator_dict = {}\n",
    "    estimator_dict2 = {}\n",
    "\n",
    "    for estimator in estimators:\n",
    "\n",
    "        if get_estimator_name(estimator) not in map_estimators.keys():\n",
    "            raise EstimatorError(\n",
    "                f\"{get_estimator_name( estimator ) } not found. Please check\"\n",
    "                \" your dict_params grid keys.\"\n",
    "            )\n",
    "        estimator_acronym = map_estimators.get(get_estimator_name(estimator))\n",
    "\n",
    "        dict_estimator_params = grid_params.get(estimator_acronym)\n",
    "        grid_dict = {}\n",
    "        grid_dict2 = {}\n",
    "        for optimizer in optimizers:\n",
    "            # get the estimator params from search estimator\n",
    "            # name\n",
    "            search_acronym = cv_seekers.get(get_estimator_name(\n",
    "                optimizer))\n",
    "            grid_param = dict_estimator_params.get(search_acronym)\n",
    "            # -------------------------------\n",
    "            cv_dict = {}\n",
    "            cv_dict2 = {}\n",
    "            for cv in cvs:\n",
    "                dict_cv_results = optimization(\n",
    "                    X, y,\n",
    "                    cv=cv,\n",
    "                    estimator=estimator,\n",
    "                    param_grid=grid_param,\n",
    "                    optimizer=optimizer,\n",
    "                    return_o=return_o,\n",
    "                    return_both=return_both,\n",
    "                    comp_name=comp_name,\n",
    "                    verbose=verbose\n",
    "                )\n",
    "\n",
    "                if return_both:\n",
    "                    dict_cv_results, dict_cv_results2 = dict_cv_results\n",
    "                    cv_dict2[f\"cv{cv}\"] = dict_cv_results2\n",
    "\n",
    "                cv_dict[f\"cv{cv}\"] = dict_cv_results\n",
    "\n",
    "            if return_o:\n",
    "                cv_box = Boxspace(** cv_dict)\n",
    "\n",
    "            else:\n",
    "                cv_box = copy.deepcopy(cv_dict)\n",
    "            # ---------------------------------\n",
    "            if return_both:\n",
    "                grid_dict2[f\"{search_acronym}\"] = cv_dict2\n",
    "\n",
    "            grid_dict[f\"{search_acronym}\"] = cv_box\n",
    "\n",
    "        if return_both:\n",
    "            estimator_dict2[f\"{estimator_acronym}\"] = grid_dict2\n",
    "\n",
    "        if return_o:\n",
    "            grid_box = Boxspace(\n",
    "                ** grid_dict\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            grid_box = copy.deepcopy(grid_dict)\n",
    "\n",
    "        estimator_dict[f\"{estimator_acronym}\"] = grid_box\n",
    "\n",
    "    estimator_o = copy.deepcopy(estimator_dict)\n",
    "\n",
    "    if return_o:\n",
    "        estimator_o = Boxspace(**estimator_dict)\n",
    "\n",
    "    if return_both:\n",
    "        estimator_o = (estimator_o, estimator_dict2)\n",
    "\n",
    "    return estimator_o\n",
    "\n",
    "\n",
    "def predict(estimator, X_test=None, y_true=None,\n",
    "            data=None, component=None,\n",
    "            verbose=False):\n",
    "\n",
    "    # ------------------assert data -----------------------------------------\n",
    "    X_test, y_true, component = _validate_data(X_test, y_true, data, component)\n",
    "    # ---------------------------------------------------------------------\n",
    "    y_pred = estimator.predict(X_test)\n",
    "    ac_score = accuracy_score(y_true, y_pred)\n",
    "    rec_score = recall_score(y_true, y_pred)\n",
    "    prec_score = precision_score(y_true, y_pred)\n",
    "    f1_scor = f1_score(y_true, y_pred)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"-\"*110)\n",
    "        print(\"{:^110}\".format(get_estimator_name(estimator)+(\n",
    "              \"''\" if not component else f\"({str(component)})\"))\n",
    "              )\n",
    "        print(\"-\"*110)\n",
    "        print(\"|{:^26}|{:^26}|{:^26}|{:^26}|\".format(\n",
    "            \"Accuracy\", \"F1_score\", \"Precision\", \"Recall\")\n",
    "        )\n",
    "        print(\"-\"*110)\n",
    "        print(\"|{:>26}|{:>26}|{:>26}|{:>26}|\".format(\n",
    "            *[round(sc, 4) for sc in (\n",
    "                ac_score, f1_scor, prec_score, rec_score)])\n",
    "              )\n",
    "        print(\"-\"*110)\n",
    "\n",
    "    dict_scores = dict(component_=component,\n",
    "                       accuracy_=ac_score,\n",
    "                       f1_score_=f1_scor,\n",
    "                       precision_=prec_score,\n",
    "                       recall_=rec_score\n",
    "                       )\n",
    "    return Boxspace(**dict_scores)\n",
    "\n",
    "\n",
    "def _validate_data(\n",
    "        X=None, y=None,  data=None, component=None):\n",
    "    \"\"\" Validate the data either from pickled dictionnary or raw X, y. \"\"\"\n",
    "    if isinstance(data, dict):\n",
    "        if not component:\n",
    "            raise ValueError(\" Missing component. Dictionnary\"\n",
    "                             \" need the component to be specified.\")\n",
    "        try:\n",
    "            regex_val = re.search(r'\\d+', str(component),\n",
    "                                  flags=re.IGNORECASE).group()\n",
    "        except:\n",
    "            raise TypeError(\"Expect a digit in the given component.\"\n",
    "                            f\" Got {component}\")\n",
    "        regex_val = int(regex_val)\n",
    "\n",
    "        Xy = data.get(f\"comp{regex_val}\")\n",
    "        if Xy is None:\n",
    "            raise ValueError(\"Component {component} does not exist in pickled\"\n",
    "                             f\" data. Valid keys are {smart_format( data.keys())}\")\n",
    "        X, y = Xy\n",
    "\n",
    "    if X is None or y is None:\n",
    "        raise TypeError(\" Need X, y\")\n",
    "\n",
    "    return X, y, component\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d701406-f510-4b3a-8d09-fa9a227830f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of the trainset and test set after the split\n",
    "\n",
    "# XXX IMPORTANT ! (3)\n",
    "dict_train_data = fetch_pickled_data(\"traindata.pca.joblib\")\n",
    "dict_test_data = fetch_pickled_data(\"testdata.pca.joblib\")\n",
    "\n",
    "# # check the dimension for components 10, 55 as examples\n",
    "\n",
    "X10train, y10train = dict_train_data.get(\"comp10\")\n",
    "X10test, y10test = dict_test_data.get(\"comp10\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45bc1c71-44b4-435e-a693-174decfddc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XXX IMPORTANT ! (4)\n",
    "\n",
    "dict_estimators = {\n",
    "    # OPTIMIZE LR\n",
    "    \"LR\": {\n",
    "        \"GSCV\": {\n",
    "            # [0.01, 0.1, 1, 10, 100],\n",
    "            'C': [np.log2(x) for x in np.arange(1, 10)],\n",
    "            'solver': ['liblinear', 'saga'],\n",
    "            'penalty': ['l1', 'l2'],  # I added\n",
    "        },\n",
    "        \"RSCV\": {\n",
    "            # np.logspace(-4, 4, 20),\n",
    "            'C': [np.log2(x) for x in np.arange(1, 10)],\n",
    "            'solver': ['liblinear', 'saga'],\n",
    "            'penalty': ['l1', 'l2'],  # I added\n",
    "        },\n",
    "        \"BSCV\": {\n",
    "            # (1e-6, 1e+6, 'log-uniform'),\n",
    "            'C': Real(np.log2(2), np.log2(10)),\n",
    "            'solver': Categorical(['liblinear', 'saga']),\n",
    "            'penalty': Categorical(['l1', 'l2']),  # I added\n",
    "        },\n",
    "    },\n",
    "    # OPTIMIZE KNN\n",
    "    \"KNN\": {\n",
    "        \"GSCV\": {\n",
    "            'n_neighbors': range(1, 31),\n",
    "            'metric': ['euclidean', 'manhattan'],\n",
    "            'weights': ['uniform', 'distance'],\n",
    "        },\n",
    "        \"RSCV\": {\n",
    "            'n_neighbors': range(1, 31),\n",
    "            'metric': ['euclidean', 'manhattan'],\n",
    "            'weights': ['uniform', 'distance'],\n",
    "        },\n",
    "        \"BSCV\": {\n",
    "            'n_neighbors': Integer(1, 31),  # (1,30)\n",
    "            'metric': Categorical(['euclidean', 'manhattan']),\n",
    "            'weights': Categorical(['uniform', 'distance']),\n",
    "        },\n",
    "    },\n",
    "    # OPTIMIZE SVM\n",
    "    \"SVM\": {\n",
    "        \"GSCV\": {\n",
    "            # [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "            'C': [np.log2(x) for x in np.arange(2, 10)],\n",
    "            'kernel': ['linear', 'rbf', 'sigmoid', 'poly'],\n",
    "            'class_weight': [None, 'balanced'],\n",
    "            'gamma': ['scale', 'auto'],\n",
    "            'degree': [1, 2, 3, 4, 5],  # Integer(1, 5),\n",
    "            'coef0': range(10)  # Real(0, 10),\n",
    "        },\n",
    "        \"RSCV\": {\n",
    "            'C': [np.log2(x) for x in np.arange(2, 10)],  # expon(scale=100),\n",
    "            'kernel': ['linear', 'rbf', 'sigmoid', 'poly'],\n",
    "            'gamma': ['scale', 'auto'],\n",
    "            'class_weight': [None, 'balanced'],\n",
    "            'degree': [1, 2, 3, 4, 5],\n",
    "            'coef0': range(10)  # [i for i in range(10)]# Real(0, 10),\n",
    "        },\n",
    "        \"BSCV\": {\n",
    "            'C': Real(np.log2(2), np.log2(10)),\n",
    "            # Real(1e-6, 1e+1, prior='log-uniform'),\n",
    "            'gamma': Categorical(['scale', 'auto']),\n",
    "            'kernel':  Categorical(['linear', 'poly', 'rbf', 'sigmoid']),\n",
    "            'degree': Integer(1, 5),\n",
    "            'coef0': Integer(1, 10),\n",
    "            'class_weight': Categorical([None, 'balanced']),\n",
    "        },\n",
    "    },\n",
    "    # OPTIMIZE XGB\n",
    "    \"XGB\": {\n",
    "        \"GSCV\": {\n",
    "            'max_depth': range(1, 30),  # [3, 5, 7, 9],\n",
    "            'learning_rate': [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3],\n",
    "            'n_estimators': [10, 20, 30],\n",
    "            'subsample': [0.7, 0.8, 0.9, 1.0],\n",
    "            'colsample_bytree': [0.7, 0.8, 0.9, 1.0],\n",
    "        },\n",
    "        \"RSCV\": {\n",
    "            # Integer(1, 30),  # [i for i in np.arange(3, 10)],\n",
    "            'max_depth': range(1, 30),\n",
    "            # np.random.uniform(0.001, 0.01, 4),\n",
    "            'learning_rate':  [0.0001, 0.001, 0.01, 0.1, 1, 10],\n",
    "            'n_estimators': [10, 20, 30],  # [np.random.randint(50, 300)],\n",
    "            # [np.random.uniform(0.7, 0.3)],\n",
    "            'subsample': [0.7, 0.8, 0.9, 1.0],\n",
    "            # np.random.uniform(0.7, 0.3, 4)\n",
    "            'colsample_bytree': [0.7, 0.8, 0.9, 1.0],\n",
    "        },\n",
    "        \"BSCV\": {\n",
    "            'max_depth':  Integer(1, 30),  # Integer(3, 10),\n",
    "            # Real(0.01, 0.3),\n",
    "            'learning_rate': Real(0.0001, 10),\n",
    "            'n_estimators': Integer(10, 30),  # Integer(50, 300),\n",
    "            # [0.7, 0.8, 0.9, 1.0],  # Real(0.7, 1.0),\n",
    "            'subsample': Real(0.7, 1.),\n",
    "            # [0.7, 0.8, 0.9, 1.0],  # Real(0.7, 1.0)\n",
    "            'colsample_bytree': Real(.7, 1.),\n",
    "        },\n",
    "    },\n",
    "    # OPTIMIZE NB\n",
    "    \"NB\": {\n",
    "        \"GSCV\": {\n",
    "            # Additive (Laplace/Lidstone) smoothing parameter\n",
    "            'var_smoothing': [i for i in np.arange(1e-4, 1e+4, 10)],\n",
    "            # Whether to learn class prior probabilities or not}\n",
    "            # 'priors': ['0', '1']\n",
    "        },\n",
    "        \"RSCV\": {\n",
    "            # Additive (Laplace/Lidstone) smoothing parameter\n",
    "            'var_smoothing': [i for i in np.arange(1e-4, 1e+4, 10)],\n",
    "            # Whether to learn class prior probabilities or not}\n",
    "            # 'priors': ['0', '1']\n",
    "        },\n",
    "        \"BSCV\": {\n",
    "            # [i for i in np.arange(1e-4, 1e+4, 10)],\n",
    "            'var_smoothing': Real(1e-4, 1e+4)\n",
    "        },\n",
    "    },\n",
    "    # OPTIMIZE DT\n",
    "    \"DT\": {\n",
    "        \"GSCV\": {\n",
    "            'max_depth': [1, 10, 20, 30],\n",
    "            'min_samples_split': range(2, 10),  # [2, 5, 8, 10],\n",
    "            'min_samples_leaf': range(1, 20),  # [1, 2, 3, 4],\n",
    "            'criterion': ['gini', 'entropy'],\n",
    "        },\n",
    "        \"RSCV\": {\n",
    "            'max_depth': range(1, 30),  # [None] + list(range(5, 55, 5)),\n",
    "            'min_samples_split': [np.random.randint(2, 10)],\n",
    "            'min_samples_leaf': [np.random.randint(1, 20)],\n",
    "            'criterion': ['gini', 'entropy'],\n",
    "        },\n",
    "        \"BSCV\": {\n",
    "            'max_depth': Integer(1, 30),\n",
    "            'min_samples_split': Integer(2, 10),\n",
    "            'min_samples_leaf': Integer(1, 20),\n",
    "            'criterion': Categorical(['gini', 'entropy']),\n",
    "        },\n",
    "    },\n",
    "    # OPTIMIZE RF\n",
    "    \"RF\": {\n",
    "        \"GSCV\": {\n",
    "            'n_estimators': [10, 20, 30],\n",
    "            'max_depth': [1, 10, 20, 30],\n",
    "            'min_samples_split': range(2, 10),  # [2, 5, 10],\n",
    "            'min_samples_leaf': range(1, 20),  # [1, 2, 4],\n",
    "            'bootstrap': [True, False],\n",
    "        },\n",
    "        \"RSCV\": {\n",
    "            'n_estimators': [10, 20, 30],  # Integer(10, 30),\n",
    "            'max_depth': range(1, 30),  # [None] + list(range(5, 55, 5)),\n",
    "            'min_samples_split': [np.random.randint(2, 4)],\n",
    "            'min_samples_leaf': [np.random.randint(1, 20)],\n",
    "            'bootstrap': [True, False],\n",
    "        },\n",
    "        \"BSCV\": {\n",
    "            'n_estimators': Integer(10, 30),  # [ 10, 20, 30],\n",
    "            'max_depth': Integer(1, 30),  # Integer(5, 50),\n",
    "            'min_samples_split': Integer(2, 10),\n",
    "            'min_samples_leaf': Integer(1, 20),\n",
    "            'bootstrap': Categorical([True, False]),\n",
    "        },\n",
    "    },\n",
    "    # OPTIMIZE LDA\n",
    "    \"LDA\": {\n",
    "        \"GSCV\": {\n",
    "            'solver': ['svd', 'lsqr', 'eigen'],\n",
    "            # 'shrinkage': [None, 'auto'],\n",
    "        },\n",
    "        \"RSCV\": {\n",
    "            'solver': ['svd', 'lsqr', 'eigen'],\n",
    "            # 'shrinkage': [None, 'auto'],\n",
    "        },\n",
    "        \"BSCV\": {\n",
    "            'solver': Categorical(['svd', 'lsqr', 'eigen']),\n",
    "            # 'shrinkage': [None, 'auto'],\n",
    "        },\n",
    "    },\n",
    "    # OPTIMIZE ADA\n",
    "    \"ADA\": {\n",
    "        \"GSCV\": {\n",
    "            'n_estimators': [10, 20, 30],\n",
    "            'learning_rate': [0.0001, 0.001, 0.01, 0.1, 1, 10],\n",
    "            'base_estimator': [DecisionTreeClassifier(max_depth=d) for d in range(1, 4)],\n",
    "        },\n",
    "        \"RSCV\": {\n",
    "            'n_estimators': [10, 20, 30],  # Integer(10, 30),\n",
    "            'learning_rate': [0.0001, 0.001, 0.01, 0.1, 1, 10],\n",
    "            'base_estimator': [DecisionTreeClassifier(max_depth=d) for d in range(1, 4)],\n",
    "        },\n",
    "        \"BSCV\": {\n",
    "            'n_estimators': Integer(10, 30),  # [10, 20, 30],\n",
    "            # [0.0001, 0.001, 0.01, 0.1, 1, 10],\n",
    "            'learning_rate': Real(1e-4, 10.),\n",
    "            'base_estimator': Categorical([DecisionTreeClassifier(max_depth=d) for d in range(1, 4)]),\n",
    "        },\n",
    "    },\n",
    "    # OPTIMIZE LGBM\n",
    "    \"LGBM\": {\n",
    "        \"GSCV\": {\n",
    "            'num_leaves': range(20, 150),   # [31, 50, 100],\n",
    "            'max_depth': [1, 10, 20, 30],\n",
    "            'learning_rate': [0.0001, 0.001, 0.01, 0.1, 1, 10],\n",
    "            'n_estimators': [10, 20, 30],\n",
    "            'boosting_type': ['gbdt', 'goss'],\n",
    "        },\n",
    "        \"RSCV\": {\n",
    "            'num_leaves': [np.random.randint(20, 150)],\n",
    "            'max_depth': [np.random.randint(1, 30)],\n",
    "            # [np.random.uniform(0.01, 0.2)],\n",
    "            'learning_rate': [0.0001, 0.001, 0.01, 0.1, 1, 10],\n",
    "            'n_estimators': [np.random.randint(10, 30)],\n",
    "            'boosting_type': ['gbdt', 'goss'],\n",
    "\n",
    "        },\n",
    "        # XXX FIT ::RFERENCE::\n",
    "        \"BSCV\": {\n",
    "            'num_leaves': Integer(20, 150),\n",
    "            'max_depth': Integer(1, 30),\n",
    "            'learning_rate':  [0.0001, 0.001, 0.01, 0.1, 1, 10],\n",
    "            'n_estimators': Integer(10, 30),\n",
    "            # 'colsample_bytree': Real(0.5, 1.0),\n",
    "            'boosting_type': Categorical(['gbdt', 'goss']),\n",
    "        },\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d377979-87de-4281-9858-e7a8c4cf2fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# XXX IMPORTANT ! (5)\n",
    "\n",
    "\n",
    "def run_metadata(\n",
    "    estimators,\n",
    "    grid_params,\n",
    "    optimizers, data,\n",
    "    cvs=(5, 10),\n",
    "    components=None,\n",
    "    save_as=None,\n",
    "    savefile='run_inv.joblib',\n",
    "    append_date=False,\n",
    "    append_versions=False,\n",
    "    verbose=False,\n",
    "):\n",
    "    \"\"\" Run Data with multiple estimators \n",
    "\n",
    "    Parameters \n",
    "    ------------\n",
    "    estimators: :class:`sklearn.Base.BaseEstimators \n",
    "       List of scikit-learn estimators or gradient boosting \n",
    "\n",
    "     grid_parameters : dict \n",
    "       DIctionnary of all estimator grid parameters for cross-validation \n",
    "\n",
    "     optimizers: :class:sklearn.validation.~`. \n",
    "       List of Sklearn optimizers. It can be ['GridSearchCV', 'RandomizedSearchCV'\n",
    "                                              'BayesSearchCV']\n",
    "     data: dict, \n",
    "       Binary Pickle or JOblib dictionnary data. \n",
    "\n",
    "     cvs: list, default =(5, 10) \n",
    "       Cross validation folds \n",
    "\n",
    "     components: list, optional \n",
    "       List of PCA component data to retrieve from the `data`. If ``None``\n",
    "       all components i.e. all data stored as binary pickled data should \n",
    "       be used instead. \n",
    "\n",
    "     save_as: str,OPtional , {'object', 'dict', 'both'), default={'dict'}\n",
    "        Save data as dictionnary of object in the binary disk. \n",
    "        Default, it saves it into a dictionnary.  if 'both', it saves \n",
    "        object and dictionnary data. \n",
    "\n",
    "    savefile: Str, default= ''run_inv.joblib' \n",
    "      The name of the file to stored the modeling results. \n",
    "\n",
    "    append_date: bool, default=False \n",
    "      Append the data  to the `savefile`.\n",
    "\n",
    "    append_versions: bool, defaut =False, \n",
    "      Append the requirement dependencies to the `savefile`.\n",
    "\n",
    "    verbose: bool, default=False \n",
    "      Control the level of verbosity. If ``False``, mute the process. \n",
    "\n",
    "    Return \n",
    "    -------\n",
    "    oo: :class:`watex.utils.box.Boxspace` \n",
    "       Modeling object created that stored all the informations during the \n",
    "       validation and modeling. \n",
    "\n",
    "\n",
    "    Examples\n",
    "    ----------\n",
    "    >>> # fetch the best estimator from the pickled \n",
    "    >>> # dictionnary data \n",
    "    >>> dict_train_data = fetch_pickled_data(\"traindata.pca.joblib\")\n",
    "    >>> # run now \n",
    "    >>> oo = run_metadata ( \n",
    "        estimators = [SVC() ], \n",
    "        optimizers =[ RandomizedSearchCV], \n",
    "        components = (10, 15 ), \n",
    "        grid_params = dict_estimators, \n",
    "        data = dict_train_data, \n",
    "        cvs= (5, 10), \n",
    "        verbose=True, \n",
    "        save_as = 'object', \n",
    "\n",
    "        )\n",
    "    >>> oo.data10.SVM.RSCV.cv5.best_estimator_\n",
    "    Out[101]: SVC(C=305.69100027133396, gamma=1e-08, kernel='linear')\n",
    "    >>> oo.data10.SVM.RSCV.cv5.best_params_\n",
    "    Out[102]: \n",
    "    {'C': 305.69100027133396,\n",
    "     'class_weight': None,\n",
    "     'gamma': 1e-08,\n",
    "     'kernel': 'linear'}\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    return_both = False\n",
    "    return_o = False\n",
    "\n",
    "    save_as = str(save_as).lower().strip()\n",
    "\n",
    "    if save_as.find('both') >= 0:\n",
    "        return_both = True\n",
    "        return_o = True\n",
    "    elif save_as.find('obj') >= 0:\n",
    "        save_as = 'object'\n",
    "        return_o = True\n",
    "\n",
    "    if not components:\n",
    "        components = list(data.keys())\n",
    "\n",
    "    components = is_iterable(components, exclude_string=True, transform=True)\n",
    "\n",
    "    data_dict = {}\n",
    "    data_dict2 = {}\n",
    "    fo = None\n",
    "    for comp in components:\n",
    "        if verbose:\n",
    "            print(\"{:-^110}\".format(f\"DATA:{comp}\"))\n",
    "        X, y, component = _validate_data(data=data, component=comp)\n",
    "\n",
    "        try:\n",
    "            o = evaluator(\n",
    "                X=X,\n",
    "                y=y,\n",
    "                # [LogisticRegression() , SVC()] ,\n",
    "                estimators=estimators,\n",
    "                grid_params=grid_params,  # dict_estimators,\n",
    "                optimizers=optimizers,  # [RandomizedSearchCV],\n",
    "                cvs=cvs,  # [5, 10] ,\n",
    "                verbose=verbose,  # rue\n",
    "                return_both=return_both,\n",
    "                return_o=return_o,\n",
    "                comp_name=\"data={}\".format(str(comp).replace(\"comp\", \"\"))\n",
    "            )\n",
    "        except:\n",
    "            o = (Boxspace(**{\"eRROR\": f\"eRROR--component={comp}\"}),\n",
    "                 {}\n",
    "                 )\n",
    "\n",
    "        if return_both:\n",
    "            o, data_dict2 = o\n",
    "\n",
    "        data_dict[f\"data{comp}\"] = o\n",
    "\n",
    "        if str(comp).find(\"55\") < 0:\n",
    "            continue\n",
    "\n",
    "    if return_o:\n",
    "        fo = Boxspace(**data_dict)\n",
    "\n",
    "    else:\n",
    "        fo = copy.deepcopy(data_dict)\n",
    "\n",
    "    file_data = dict(datao=fo, data=data_dict, data2=data_dict2)\n",
    "\n",
    "    if savefile:\n",
    "        savejob(file_data,\n",
    "                savefile=savefile,\n",
    "                append_date=append_date,\n",
    "                append_versions=append_versions,\n",
    "                )\n",
    "    return fo\n",
    "\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65d69ab6-3a7d-46d6-bd96-ba61b8ee8d14",
   "metadata": {},
   "outputs": [],
   "source": [
    " # XXX IMPORTANT ! (6)\n",
    "\n",
    "dict_train_data = fetch_pickled_data(\"traindata.pca.joblib\")\n",
    "# # %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3dfaa5a7-fd62-4675-ba6e-e082a252145d",
   "metadata": {},
   "outputs": [],
   "source": [
    " # XXX IMPORTANT ! (7)\n",
    "# run all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d79e5cca-e636-411e-a8fa-8cf183863a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BZ\\.conda\\envs\\DanP\\lib\\site-packages\\sklearn\\base.py:299: UserWarning: Trying to unpickle estimator SVC from version 1.0.2 when using version 1.2.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\BZ\\.conda\\envs\\DanP\\lib\\site-packages\\sklearn\\base.py:299: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.0.2 when using version 1.2.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\BZ\\.conda\\envs\\DanP\\lib\\site-packages\\sklearn\\base.py:299: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 1.0.2 when using version 1.2.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\BZ\\.conda\\envs\\DanP\\lib\\site-packages\\sklearn\\base.py:299: UserWarning: Trying to unpickle estimator AdaBoostClassifier from version 1.0.2 when using version 1.2.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\BZ\\.conda\\envs\\DanP\\lib\\site-packages\\sklearn\\base.py:299: UserWarning: Trying to unpickle estimator KNeighborsClassifier from version 1.0.2 when using version 1.2.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\BZ\\.conda\\envs\\DanP\\lib\\site-packages\\sklearn\\base.py:299: UserWarning: Trying to unpickle estimator RandomForestClassifier from version 1.0.2 when using version 1.2.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\BZ\\.conda\\envs\\DanP\\lib\\site-packages\\sklearn\\base.py:299: UserWarning: Trying to unpickle estimator GaussianNB from version 1.0.2 when using version 1.2.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\BZ\\.conda\\envs\\DanP\\lib\\site-packages\\sklearn\\base.py:299: UserWarning: Trying to unpickle estimator LinearDiscriminantAnalysis from version 1.0.2 when using version 1.2.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\BZ\\.conda\\envs\\DanP\\lib\\site-packages\\sklearn\\base.py:299: UserWarning: Trying to unpickle estimator LabelEncoder from version 1.0.2 when using version 1.2.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "DATA = joblib.load(\"ModResults.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6394976-c000-4a84-bc82-fd7ddffbb4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "datao = DATA['datao']\n",
    "#     )\n",
    "\n",
    "def display_results(data_dict, stop='10'):\n",
    "    map_estimators = {\n",
    "        \"LogisticRegression\": \"LR\",\n",
    "        'KNeighborsClassifier': 'KNN',\n",
    "        'DecisionTreeClassifier': 'DT',\n",
    "        'SVC': \"SVM\",\n",
    "        'RandomForestClassifier': \"RF\",\n",
    "        'AdaBoostClassifier': \"ADA\",\n",
    "        'XGBClassifier': \"XGB\",\n",
    "        'GaussianNB': \"NB\",\n",
    "        \"LGBMClassifier\": \"LGBM\",\n",
    "        \"LinearDiscriminantAnalysis\": \"LDA\",\n",
    "        # \"Ridge\": \"Ridge\"\n",
    "    }\n",
    "    maincomp = [10, 15, 20, 25, 30, 35, 40, 45, 50]\n",
    "    components = [f'comp{i}' for i in maincomp]\n",
    "    \n",
    "    if stop is not None: \n",
    "        components = is_in_if ( [f\"comp{stop}\"], components, return_intersect=True)\n",
    "    if components is None: \n",
    "        raise TypeError(f\"data {stop} is not available. Expect {smart_format(maincomp)}\")\n",
    "    est_keys = list(map_estimators.values())\n",
    "    cvs = ['cv5', 'cv10']\n",
    "    grids = ['RSCV', 'GSCV', 'BSCV']\n",
    "    datao = data_dict['datao']\n",
    "\n",
    "    print(\"+\"*320)\n",
    "    print(\"{:^15} | {:^15} | {:^5} | {:^5} | {:^150} | {:^40} | {:^80}\".format(\n",
    "        \"DATA\", \"Estimators\", \"Grid\", \"CV\",  \"best_params\",  \"Train scores (acc)\", \"Test scores\"))\n",
    "    print(\"+\"*320)\n",
    "    for comp in components:\n",
    "        data = f\"data{comp}\"\n",
    "        for estimator in est_keys:\n",
    "            for grid in grids:\n",
    "                for cv in cvs:\n",
    "                    cv_obj = getattr(\n",
    "                        getattr(getattr(getattr(datao, data), estimator), grid), cv)\n",
    "                    best_estimator = cv_obj.best_estimator_\n",
    "                    best_params = cv_obj.best_params_\n",
    "                    ttest_scores, std_scores = getGlobalScores(\n",
    "                        cv_obj.cv_results_)\n",
    "                    po = predict(best_estimator,\n",
    "                                 data=dict_test_data, component=comp)\n",
    "\n",
    "                    print(\"{:^15} | {:^15} | {:^5} | {:^5} | {:^150} | {:^40} | {:^80}\".format(\n",
    "                        data, estimator, grid,  cv,  str(best_params),\n",
    "                        \"{:<5}-{:>5}\".format(round(ttest_scores, 3),\n",
    "                                             round(std_scores, 3)),\n",
    "                        \"acc:{:<10} - precision:{:<10} - recall {:<10} - f1_score: {:<10}\".format(\n",
    "                            round(po.accuracy_, 4), round(\n",
    "                                po.precision_, 4), round(po.recall_, 4),\n",
    "                            round(po.f1_score_, 4))\n",
    "                    )\n",
    "                    )\n",
    "\n",
    "        print(\"-\"*320)\n",
    "        if str(stop) in comp:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09aad127-4e2b-424e-9440-37362291918f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "     DATA       |   Estimators    | Grid  |  CV   |                                                                      best_params                                                                       |            Train scores (acc)            |                                   Test scores                                   \n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "  datacomp10    |       LR        | RSCV  |  cv5  |                                              {'solver': 'saga', 'penalty': 'l1', 'C': 2.321928094887362}                                               |               nan  -  nan                | acc:0.8681     - precision:0.8857     - recall 0.9394     - f1_score: 0.9118    \n",
      "  datacomp10    |       LR        | RSCV  | cv10  |                                              {'solver': 'saga', 'penalty': 'l2', 'C': 2.321928094887362}                                               |               0.863-0.072                | acc:0.8681     - precision:0.8857     - recall 0.9394     - f1_score: 0.9118    \n",
      "  datacomp10    |       LR        | GSCV  |  cv5  |                                              {'C': 1.584962500721156, 'penalty': 'l1', 'solver': 'saga'}                                               |               nan  -  nan                | acc:0.8571     - precision:0.8732     - recall 0.9394     - f1_score: 0.9051    \n",
      "  datacomp10    |       LR        | GSCV  | cv10  |                                                   {'C': 1.0, 'penalty': 'l1', 'solver': 'liblinear'}                                                   |               nan  -  nan                | acc:0.8571     - precision:0.8732     - recall 0.9394     - f1_score: 0.9051    \n",
      "  datacomp10    |       LR        | BSCV  |  cv5  |                                  OrderedDict([('C', 3.207933310825696), ('penalty', 'l1'), ('solver', 'liblinear')])                                   |               0.853-0.046                | acc:0.8681     - precision:0.8857     - recall 0.9394     - f1_score: 0.9118    \n",
      "  datacomp10    |       LR        | BSCV  | cv10  |                                  OrderedDict([('C', 1.1325340919202165), ('penalty', 'l1'), ('solver', 'liblinear')])                                  |               0.862-0.072                | acc:0.8571     - precision:0.8732     - recall 0.9394     - f1_score: 0.9051    \n",
      "  datacomp10    |       KNN       | RSCV  |  cv5  |                                            {'weights': 'uniform', 'n_neighbors': 26, 'metric': 'manhattan'}                                            |               0.791-0.079                | acc:0.8352     - precision:0.8493     - recall 0.9394     - f1_score: 0.8921    \n",
      "  datacomp10    |       KNN       | RSCV  | cv10  |                                            {'weights': 'uniform', 'n_neighbors': 7, 'metric': 'manhattan'}                                             |               0.788-0.093                | acc:0.8462     - precision:0.8421     - recall 0.9697     - f1_score: 0.9014    \n",
      "  datacomp10    |       KNN       | GSCV  |  cv5  |                                           {'metric': 'euclidean', 'n_neighbors': 24, 'weights': 'distance'}                                            |               0.786-0.072                | acc:0.8571     - precision:0.863      - recall 0.9545     - f1_score: 0.9065    \n",
      "  datacomp10    |       KNN       | GSCV  | cv10  |                                            {'metric': 'euclidean', 'n_neighbors': 18, 'weights': 'uniform'}                                            |               0.797-0.094                | acc:0.8681     - precision:0.875      - recall 0.9545     - f1_score: 0.913     \n",
      "  datacomp10    |       KNN       | BSCV  |  cv5  |                                  OrderedDict([('metric', 'euclidean'), ('n_neighbors', 31), ('weights', 'uniform')])                                   |               0.779-0.069                | acc:0.8571     - precision:0.8533     - recall 0.9697     - f1_score: 0.9078    \n",
      "  datacomp10    |       KNN       | BSCV  | cv10  |                                  OrderedDict([('metric', 'euclidean'), ('n_neighbors', 26), ('weights', 'distance')])                                  |               0.8  -0.092                | acc:0.8791     - precision:0.8767     - recall 0.9697     - f1_score: 0.9209    \n",
      "  datacomp10    |       DT        | RSCV  |  cv5  |                                {'min_samples_split': 6, 'min_samples_leaf': 2, 'max_depth': 4, 'criterion': 'entropy'}                                 |               0.755-0.041                | acc:0.7692     - precision:0.8814     - recall 0.7879     - f1_score: 0.832     \n",
      "  datacomp10    |       DT        | RSCV  | cv10  |                                  {'min_samples_split': 6, 'min_samples_leaf': 2, 'max_depth': 3, 'criterion': 'gini'}                                  |               0.769-0.073                | acc:0.7802     - precision:0.9259     - recall 0.7576     - f1_score: 0.8333    \n",
      "  datacomp10    |       DT        | GSCV  |  cv5  |                                 {'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 13, 'min_samples_split': 2}                                 |               0.775-0.049                | acc:0.8132     - precision:0.8769     - recall 0.8636     - f1_score: 0.8702    \n",
      "  datacomp10    |       DT        | GSCV  | cv10  |                                 {'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 16, 'min_samples_split': 3}                                 |               0.78 - 0.07                | acc:0.8132     - precision:0.8769     - recall 0.8636     - f1_score: 0.8702    \n",
      "  datacomp10    |       DT        | BSCV  |  cv5  |                     OrderedDict([('criterion', 'entropy'), ('max_depth', 3), ('min_samples_leaf', 10), ('min_samples_split', 4)])                      |               0.775- 0.05                | acc:0.7692     - precision:0.8947     - recall 0.7727     - f1_score: 0.8293    \n",
      "  datacomp10    |       DT        | BSCV  | cv10  |                     OrderedDict([('criterion', 'entropy'), ('max_depth', 8), ('min_samples_leaf', 15), ('min_samples_split', 7)])                      |               0.777-0.068                | acc:0.8132     - precision:0.8769     - recall 0.8636     - f1_score: 0.8702    \n",
      "  datacomp10    |       SVM       | RSCV  |  cv5  |                             {'kernel': 'linear', 'gamma': 'auto', 'degree': 1, 'coef0': 9, 'class_weight': None, 'C': 2.0}                             |               0.788-0.042                | acc:0.8681     - precision:0.8857     - recall 0.9394     - f1_score: 0.9118    \n",
      "  datacomp10    |       SVM       | RSCV  | cv10  |                      {'kernel': 'linear', 'gamma': 'auto', 'degree': 3, 'coef0': 2, 'class_weight': None, 'C': 1.584962500721156}                      |               0.789-0.054                | acc:0.8791     - precision:0.8986     - recall 0.9394     - f1_score: 0.9185    \n",
      "  datacomp10    |       SVM       | GSCV  |  cv5  |                            {'C': 1.0, 'class_weight': None, 'coef0': 1, 'degree': 1, 'gamma': 'scale', 'kernel': 'sigmoid'}                            |               0.784-0.054                | acc:0.8462     - precision:0.8714     - recall 0.9242     - f1_score: 0.8971    \n",
      "  datacomp10    |       SVM       | GSCV  | cv10  |                             {'C': 2.0, 'class_weight': None, 'coef0': 2, 'degree': 2, 'gamma': 'scale', 'kernel': 'poly'}                              |               0.776- 0.07                | acc:0.8571     - precision:0.8841     - recall 0.9242     - f1_score: 0.9037    \n",
      "  datacomp10    |       SVM       | BSCV  |  cv5  |          OrderedDict([('C', 1.2704824865520012), ('class_weight', None), ('coef0', 6), ('degree', 4), ('gamma', 'scale'), ('kernel', 'rbf')])          |               0.785-0.046                | acc:0.8571     - precision:0.8732     - recall 0.9394     - f1_score: 0.9051    \n",
      "  datacomp10    |       SVM       | BSCV  | cv10  |         OrderedDict([('C', 1.0663464118176944), ('class_weight', None), ('coef0', 1), ('degree', 4), ('gamma', 'auto'), ('kernel', 'linear')])         |               0.794- 0.07                | acc:0.8791     - precision:0.8986     - recall 0.9394     - f1_score: 0.9185    \n",
      "  datacomp10    |       RF        | RSCV  |  cv5  |                        {'n_estimators': 30, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_depth': 18, 'bootstrap': True}                         |               0.793-0.069                | acc:0.8352     - precision:0.8696     - recall 0.9091     - f1_score: 0.8889    \n",
      "  datacomp10    |       RF        | RSCV  | cv10  |                        {'n_estimators': 30, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_depth': 21, 'bootstrap': True}                         |               0.793-0.076                | acc:0.7582     - precision:0.8056     - recall 0.8788     - f1_score: 0.8406    \n",
      "  datacomp10    |       RF        | GSCV  |  cv5  |                        {'bootstrap': True, 'max_depth': 10, 'min_samples_leaf': 3, 'min_samples_split': 6, 'n_estimators': 20}                         |               0.776-0.045                | acc:0.7802     - precision:0.7875     - recall 0.9545     - f1_score: 0.863     \n",
      "  datacomp10    |       RF        | GSCV  | cv10  |                        {'bootstrap': True, 'max_depth': 20, 'min_samples_leaf': 4, 'min_samples_split': 9, 'n_estimators': 30}                         |               0.782-0.057                | acc:0.8132     - precision:0.8182     - recall 0.9545     - f1_score: 0.8811    \n",
      "  datacomp10    |       RF        | BSCV  |  cv5  |            OrderedDict([('bootstrap', True), ('max_depth', 15), ('min_samples_leaf', 15), ('min_samples_split', 5), ('n_estimators', 12)])             |               0.798-0.055                | acc:0.8462     - precision:0.8421     - recall 0.9697     - f1_score: 0.9014    \n",
      "  datacomp10    |       RF        | BSCV  | cv10  |            OrderedDict([('bootstrap', False), ('max_depth', 15), ('min_samples_leaf', 6), ('min_samples_split', 9), ('n_estimators', 20)])             |               0.797-0.065                | acc:0.7582     - precision:0.8143     - recall 0.8636     - f1_score: 0.8382    \n",
      "  datacomp10    |       ADA       | RSCV  |  cv5  |                           {'n_estimators': 20, 'learning_rate': 0.01, 'base_estimator': DecisionTreeClassifier(max_depth=2)}                           |               0.78 -0.044                | acc:0.7692     - precision:0.8814     - recall 0.7879     - f1_score: 0.832     \n",
      "  datacomp10    |       ADA       | RSCV  | cv10  |                            {'n_estimators': 20, 'learning_rate': 1, 'base_estimator': DecisionTreeClassifier(max_depth=1)}                             |               0.658-0.091                | acc:0.8462     - precision:0.8514     - recall 0.9545     - f1_score: 0.9       \n",
      "  datacomp10    |       ADA       | GSCV  |  cv5  |                           {'base_estimator': DecisionTreeClassifier(max_depth=2), 'learning_rate': 0.01, 'n_estimators': 20}                           |               0.738-0.055                | acc:0.7692     - precision:0.8814     - recall 0.7879     - f1_score: 0.832     \n",
      "  datacomp10    |       ADA       | GSCV  | cv10  |                            {'base_estimator': DecisionTreeClassifier(max_depth=1), 'learning_rate': 1, 'n_estimators': 30}                             |               0.733-0.071                | acc:0.8022     - precision:0.8333     - recall 0.9091     - f1_score: 0.8696    \n",
      "  datacomp10    |       ADA       | BSCV  |  cv5  |           OrderedDict([('base_estimator', DecisionTreeClassifier(max_depth=3)), ('learning_rate', 8.217196238249068), ('n_estimators', 23)])           |               0.454-0.077                | acc:0.7473     - precision:0.759      - recall 0.9545     - f1_score: 0.8456    \n",
      "  datacomp10    |       ADA       | BSCV  | cv10  |          OrderedDict([('base_estimator', DecisionTreeClassifier(max_depth=2)), ('learning_rate', 0.11736642076394711), ('n_estimators', 12)])          |               0.701- 0.09                | acc:0.7912     - precision:0.873      - recall 0.8333     - f1_score: 0.8527    \n",
      "  datacomp10    |       XGB       | RSCV  |  cv5  |                         {'subsample': 0.7, 'n_estimators': 30, 'max_depth': 5, 'learning_rate': 0.01, 'colsample_bytree': 0.7}                         |               0.749-0.083                | acc:0.7912     - precision:0.8406     - recall 0.8788     - f1_score: 0.8593    \n",
      "  datacomp10    |       XGB       | RSCV  | cv10  |                        {'subsample': 0.7, 'n_estimators': 20, 'max_depth': 14, 'learning_rate': 0.001, 'colsample_bytree': 1.0}                        |               0.739-0.094                | acc:0.8022     - precision:0.8636     - recall 0.8636     - f1_score: 0.8636    \n",
      "  datacomp10    |       XGB       | GSCV  |  cv5  |                        {'colsample_bytree': 0.9, 'learning_rate': 0.0001, 'max_depth': 5, 'n_estimators': 20, 'subsample': 0.8}                        |               0.805-0.057                | acc:0.8132     - precision:0.8551     - recall 0.8939     - f1_score: 0.8741    \n",
      "  datacomp10    |       XGB       | GSCV  | cv10  |                         {'colsample_bytree': 0.7, 'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 30, 'subsample': 0.8}                          |               0.805-0.073                | acc:0.8022     - precision:0.8529     - recall 0.8788     - f1_score: 0.8657    \n",
      "  datacomp10    |       XGB       | BSCV  |  cv5  | OrderedDict([('colsample_bytree', 0.870852309721103), ('learning_rate', 0.48879614287152306), ('max_depth', 8), ('n_estimators', 16), ('subsample', 0.9091390719851412)]) |               0.643-0.127                | acc:0.8132     - precision:0.8356     - recall 0.9242     - f1_score: 0.8777    \n",
      "  datacomp10    |       XGB       | BSCV  | cv10  | OrderedDict([('colsample_bytree', 0.8748154665027891), ('learning_rate', 0.020116808987780614), ('max_depth', 17), ('n_estimators', 26), ('subsample', 0.8947347955819325)]) |               0.668-0.128                | acc:0.7802     - precision:0.8382     - recall 0.8636     - f1_score: 0.8507    \n",
      "  datacomp10    |       NB        | RSCV  |  cv5  |                                                              {'var_smoothing': 1750.0001}                                                              |               0.708-0.008                | acc:0.7253     - precision:0.7253     - recall 1.0        - f1_score: 0.8408    \n",
      "  datacomp10    |       NB        | RSCV  | cv10  |                                                              {'var_smoothing': 9250.0001}                                                              |               0.708-0.013                | acc:0.7253     - precision:0.7253     - recall 1.0        - f1_score: 0.8408    \n",
      "  datacomp10    |       NB        | GSCV  |  cv5  |                                                               {'var_smoothing': 0.0001}                                                                |               0.708-0.008                | acc:0.8242     - precision:0.8676     - recall 0.8939     - f1_score: 0.8806    \n",
      "  datacomp10    |       NB        | GSCV  | cv10  |                                                               {'var_smoothing': 0.0001}                                                                |               0.708-0.013                | acc:0.8242     - precision:0.8676     - recall 0.8939     - f1_score: 0.8806    \n",
      "  datacomp10    |       NB        | BSCV  |  cv5  |                                                  OrderedDict([('var_smoothing', 4123.536678732167)])                                                   |               0.708-0.008                | acc:0.7253     - precision:0.7253     - recall 1.0        - f1_score: 0.8408    \n",
      "  datacomp10    |       NB        | BSCV  | cv10  |                                                  OrderedDict([('var_smoothing', 2550.341458065758)])                                                   |               0.708-0.013                | acc:0.7253     - precision:0.7253     - recall 1.0        - f1_score: 0.8408    \n",
      "  datacomp10    |      LGBM       | RSCV  |  cv5  |                        {'num_leaves': 148, 'n_estimators': 27, 'max_depth': 15, 'learning_rate': 0.1, 'boosting_type': 'gbdt'}                         |               0.714-0.043                | acc:0.8352     - precision:0.84       - recall 0.9545     - f1_score: 0.8936    \n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "  datacomp10    |      LGBM       | RSCV  | cv10  |                        {'num_leaves': 148, 'n_estimators': 27, 'max_depth': 15, 'learning_rate': 0.1, 'boosting_type': 'goss'}                         |               0.729- 0.06                | acc:0.8352     - precision:0.84       - recall 0.9545     - f1_score: 0.8936    \n",
      "  datacomp10    |      LGBM       | GSCV  |  cv5  |                          {'boosting_type': 'gbdt', 'learning_rate': 1, 'max_depth': 1, 'n_estimators': 30, 'num_leaves': 20}                           |               0.704-0.043                | acc:0.8242     - precision:0.8289     - recall 0.9545     - f1_score: 0.8873    \n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "  datacomp10    |      LGBM       | GSCV  | cv10  |                         {'boosting_type': 'goss', 'learning_rate': 0.1, 'max_depth': 10, 'n_estimators': 30, 'num_leaves': 20}                         |               0.709-0.057                | acc:0.8462     - precision:0.8514     - recall 0.9545     - f1_score: 0.9       \n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "  datacomp10    |      LGBM       | BSCV  |  cv5  |              OrderedDict([('boosting_type', 'goss'), ('learning_rate', 1.0), ('max_depth', 3), ('n_estimators', 27), ('num_leaves', 52)])              |               0.714-0.059                | acc:0.8242     - precision:0.8788     - recall 0.8788     - f1_score: 0.8788    \n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "  datacomp10    |      LGBM       | BSCV  | cv10  |             OrderedDict([('boosting_type', 'goss'), ('learning_rate', 0.1), ('max_depth', 28), ('n_estimators', 17), ('num_leaves', 72)])              |               0.728-0.052                | acc:0.8242     - precision:0.8378     - recall 0.9394     - f1_score: 0.8857    \n",
      "  datacomp10    |       LDA       | RSCV  |  cv5  |                                                                   {'solver': 'svd'}                                                                    |               0.835-0.045                | acc:0.8791     - precision:0.8873     - recall 0.9545     - f1_score: 0.9197    \n",
      "  datacomp10    |       LDA       | RSCV  | cv10  |                                                                   {'solver': 'svd'}                                                                    |               0.84 -0.083                | acc:0.8791     - precision:0.8873     - recall 0.9545     - f1_score: 0.9197    \n",
      "  datacomp10    |       LDA       | GSCV  |  cv5  |                                                                   {'solver': 'svd'}                                                                    |               0.835-0.045                | acc:0.8791     - precision:0.8873     - recall 0.9545     - f1_score: 0.9197    \n",
      "  datacomp10    |       LDA       | GSCV  | cv10  |                                                                   {'solver': 'svd'}                                                                    |               0.84 -0.083                | acc:0.8791     - precision:0.8873     - recall 0.9545     - f1_score: 0.9197    \n",
      "  datacomp10    |       LDA       | BSCV  |  cv5  |                                                            OrderedDict([('solver', 'svd')])                                                            |               0.835-0.045                | acc:0.8791     - precision:0.8873     - recall 0.9545     - f1_score: 0.9197    \n",
      "  datacomp10    |       LDA       | BSCV  | cv10  |                                                           OrderedDict([('solver', 'lsqr')])                                                            |               0.84 -0.083                | acc:0.8791     - precision:0.8873     - recall 0.9545     - f1_score: 0.9197    \n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "display_results(DATA, stop = 10) # The code works well. Change stop and put one of these [10,15,20,25,30,35,40,45,50] and get results \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520f638a-3d67-44bc-92ff-1ee2a5b175fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4d85b5-0351-4b42-9134-589fcfc4f2c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
